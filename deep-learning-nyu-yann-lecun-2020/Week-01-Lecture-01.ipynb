{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3dfb3d2",
   "metadata": {},
   "source": [
    "# Week 01 Lecture 01: History, Motivation, and Evolution of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124dc238",
   "metadata": {},
   "source": [
    "## 1. History and Evolution of Deep Learning\n",
    "### 1940-1980s\n",
    "#### The idea of mimicking neurons in human brain started in 1940s (called Cybernetics then).\n",
    "#### Earlier experiments like Perceptron etc. used Binary Neurons. Binary Neurons never needed multiplication with weights. We had to simply add the weights for ON Neurons and discard weights corresponding to OFF neurons. Also, since floating point operations were feasible only in expensive computers till 1980s, Continuous Neurons (which are differentiable and hence we can apply Gradient Descent) were not at all considered a possibility till 1980s.\n",
    "#### Since we cannot do much other than some basic pattern recognition with proposed methods for Binary Neurons, interest in Cybernetics died out at the end of 1960s. Between end of 1960s to 1984, nobody was working in Neural Networks except some isolated researchers mostly in Japan.\n",
    "\n",
    "### 1985 - Backpropagation Algorithm\n",
    "#### By 1980s, floating point operations were feasible even for general purpose computers and the idea of using Continuous Neurons (i.e. Neurons using a Continuous Activiation Function) emerged. Backpropagation Algorithm can be used to perform Gradient Descent on such Continuous Neurons. This wave of interest in Neural Networks lasted about 10 years from 1985 to 1995 and died out again.\n",
    "\n",
    "### 2010 Onwards\n",
    "#### Around 2009 or 2010, people realized that Multilayer Neural Nets can be trained with Backprop and resulted in an improvement for Speech Recognition over traditional Hand-crafted methods. Within 2 years, almost every major player in Speech Recognition used Neural Nets and most of the Speech Recognition technology in Android phones used Neural Nets by 2012. That was the first world-wide deployment of modern forms of Deep Learning.\n",
    "#### Around the end of 2012 and early 2013, Computer Vision community realized that Deep Learning (Convolutional Nets in particular) works much better than traditional Computer Vision techniques that depended on Hand-crafted Features. That created a 2nd revolution in Computer Vision.\n",
    "#### Around 2015-2016, the same thing happened in Natural Language Processing.\n",
    "#### Now, we are going to see the same revolution in Robotics, Control and a whole bunch of application areas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fc0154",
   "metadata": {},
   "source": [
    "## 2. Supervised Learning\n",
    "![Traditional Machine Learning Vs Deep Learning](images/week-01-lecture-01-image-01-traditional-ml-vs-deep-learning.png \"Traditional Machine Learning Vs Deep Learning\")\n",
    "\n",
    "#### Deep Neural Networks are a cascade of non-linear layers/modules. Each non-linear layer/module can be represented as the cascade of a matrix and a non-linear activation function. It takes a vector as input. The no of columns of matrix should be equal to no of rows of input column vector. Then, the output after matrix vector multiplication will be a column vector with no of rows equal to no of rows of matrix. A non-linear activation function (eg: ReLU) is applied to this output vector and the output of the activation function (which is also a column vector with no of rows equal to no of rows of matrix) will be the input of the next non-linear layer/module.\n",
    "\n",
    "![Formation of Multilayer Neural Nets](images/week-01-lecture-01-image-02-formation-of-multilayer-neural-nets.png \"Formation of Multilayer Neural Nets\")\n",
    "\n",
    "#### In Supervised Learning, our training data consists of inputs and expected outputs. Using Backpropagation Algorithm, parameters in different layers of Neural Network are adjusted so that the outputs produced by the Neural Net converges to the expected outputs. We have to compute the difference between expected and actual outputs (for example, Euclidean distance between expected & actual vectors) for each training sample. The Objective Function that we want to minimize wrt to the parameters of the Neural Net is the average of the difference computed over the training set i.e. we want to find values of parameters that minimizes the average error between expected & actual outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yann_env38",
   "language": "python",
   "name": "yann_env38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
