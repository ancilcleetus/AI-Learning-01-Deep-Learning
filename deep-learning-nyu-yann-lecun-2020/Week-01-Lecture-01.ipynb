{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3dfb3d2",
   "metadata": {},
   "source": [
    "# Week 01 Lecture 01: History, Motivation, and Evolution of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124dc238",
   "metadata": {},
   "source": [
    "## 1. History and Evolution of Deep Learning\n",
    "### 1940-1980s\n",
    "The idea of mimicking neurons in human brain started in 1940s (called Cybernetics then).\n",
    "\n",
    "Earlier experiments like Perceptron etc. used Binary Neurons. Binary Neurons never needed multiplication with weights. We had to simply add the weights for ON Neurons and discard weights corresponding to OFF neurons. Also, since floating point operations were feasible only in expensive computers till 1980s, Continuous Neurons (which are differentiable and hence we can apply Gradient Descent) were not at all considered a possibility till 1980s.\n",
    "\n",
    "Since we cannot do much other than some basic pattern recognition with proposed methods for Binary Neurons, interest in Cybernetics died out at the end of 1960s. Between end of 1960s to 1984, nobody was working in Neural Networks except some isolated researchers mostly in Japan.\n",
    "\n",
    "### 1985 - Backpropagation Algorithm\n",
    "By 1980s, floating point operations were feasible even for general purpose computers and the idea of using Continuous Neurons (i.e. Neurons using a Continuous Activiation Function) emerged. Backpropagation Algorithm can be used to perform Gradient Descent on such Continuous Neurons. This wave of interest in Neural Networks lasted about 10 years from 1985 to 1995 and died out again.\n",
    "\n",
    "### 2010 Onwards\n",
    "Around 2009 or 2010, people realized that Multilayer Neural Nets can be trained with Backprop and resulted in an improvement for Speech Recognition over traditional Hand-crafted methods. Within 2 years, almost every major player in Speech Recognition used Neural Nets and most of the Speech Recognition technology in Android phones used Neural Nets by 2012. That was the first world-wide deployment of modern forms of Deep Learning.\n",
    "\n",
    "Around the end of 2012 and early 2013, Computer Vision community realized that Deep Learning (Convolutional Nets in particular) works much better than traditional Computer Vision techniques that depended on Hand-crafted Features. That created a 2nd revolution in Computer Vision.\n",
    "\n",
    "Around 2015-2016, the same thing happened in Natural Language Processing.\n",
    "\n",
    "Now, we are going to see the same revolution in Robotics, Control and a whole bunch of application areas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e9fde9",
   "metadata": {},
   "source": [
    "## 2. Deep Neural Networks\n",
    "\n",
    "![Traditional Machine Learning Vs Deep Learning](images/week-01-lecture-01-image-01-traditional-ml-vs-deep-learning.png \"Traditional Machine Learning Vs Deep Learning\")\n",
    "\n",
    "Deep Neural Networks are a cascade of non-linear layers/modules. Each non-linear layer/module can be represented as the cascade of a matrix and a non-linear activation function. It takes a vector as input. The no of columns of matrix should be equal to no of rows of input column vector. Then, the output after matrix vector multiplication will be a column vector with no of rows equal to no of rows of matrix. A non-linear activation function (eg: ReLU) is applied to this output vector and the output of the activation function (which is also a column vector with no of rows equal to no of rows of matrix) will be the input of the next non-linear layer/module.\n",
    "\n",
    "![Formation of Multilayer Neural Nets](images/week-01-lecture-01-image-02-formation-of-multilayer-neural-nets.png \"Formation of Multilayer Neural Nets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fc0154",
   "metadata": {},
   "source": [
    "## 3. Supervised Learning\n",
    "\n",
    "In Supervised Learning, our training data consists of inputs and expected outputs. Using Backpropagation Algorithm, parameters in different layers of Neural Network are adjusted so that the outputs produced by the Neural Net converges to the expected outputs. We have to compute the difference between expected and actual outputs (for example, Euclidean distance between expected & actual vectors) for each training sample. The Objective Function that we want to minimize w.r.t to the parameters of the Neural Net is the average of the difference computed over the training set i.e. we want to find values of parameters that minimizes the average error between expected & actual outputs.\n",
    "\n",
    "![Objective Function Optimization in Supervised Learning](images/week-01-lecture-01-image-03-function-optimization-in-supervised-learning.png \"Objective Function Optimization in Supervised Learning\")\n",
    "\n",
    "In true Gradient Descent, we compute the Objective Function by taking gradient over the entire training set. But it is more efficient to take one sample or a small batch of samples, computing the error made by that sample, then computing the gradient of that error w.r.t to the parameters and then taking a small step. For the next sample, the step may be in another direction since its a different sample. So, in this case, you will be going down in a noisy way with lots of fluctuations. This is called Stochastic Gradient Descent. Stochastic Gradient Descent converges much much faster compared to Gradient Descent, particularly if you have a large training set. You also get better generalization with Stochastic Gradient Descent in the end once you do inference on a test set. Stochastic Gradient Descent also takes advantage of the redundancy in training set while Gradient Descent averages over the entire training set without checking whether there is redundant data in the training set. In the real scenario, many samples are similar and hence there is a high percent of redundant data even if there is no repetition of same samples. Hence Stochastic Gradient Descent is more efficient and results in an even better overall generalized model, even though it is noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3be04a",
   "metadata": {},
   "source": [
    "## 4. Backpropagation Algorithm\n",
    "\n",
    "![Computing Gradients by Backpropagation](images/week-01-lecture-01-image-04-computing-gradients-by-backpropagation.png \"Computing Gradients by Backpropagation\")\n",
    "\n",
    "Backpropagation Algorithm is a practical application of Chain Rule in Calculus. If you want to compute the derivative of the difference between expected output and actual output (i.e. value of Objective function) w.r.t any parameter of the Neural Network, then you have to propagate derivatives backwards and multiply on the way. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yann_env38",
   "language": "python",
   "name": "yann_env38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
